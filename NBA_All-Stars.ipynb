{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the NBA All-Stars with Machine Learning\n",
    "\n",
    "Goal of the analysis is to predict the NBA All-Stars for a given year, based on All-Star selections in other years. This is accomplished by testing several machine learning classification algorithms and player performance statistics per season. The analysis in this Jupyter notebook is based on the [Scikit-learn](http://scikit-learn.org) machine learning package, NBA player data are taken from https://www.basketball-reference.com.\n",
    "\n",
    "## Analysis initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NBAanalysissetup # See NBAanalysissetup.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the year you want to use for validation and the year you want to predict (both in range 2000-2018), and if you want to include advanced players statistics (e.g. PER, WS, etc.) or not. The years in range 2000-2018 that are not selected as validation and prediction years are used as training data for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_years = list(range(2000, 2019))\n",
    "\n",
    "validation_year = 2017 # Enter any year from 2000 until 2018\n",
    "prediction_year = 2018 # Enter any year from 2000 until 2018\n",
    "    \n",
    "training_years.remove(validation_year)\n",
    "training_years.remove(prediction_year)\n",
    "    \n",
    "first_training_year = training_years[0]\n",
    "last_training_year  = training_years[-1]\n",
    "    \n",
    "includeadvancedstats = True # Enter True or False\n",
    "\n",
    "print(\"--> Validation_year = {}\".format(validation_year))\n",
    "print(\"--> Prediction_year = {}\".format(prediction_year))\n",
    "if (first_training_year < validation_year < last_training_year) and (first_training_year < prediction_year < last_training_year):\n",
    "    print(\"--> Training years  = {}-{} except {} and {}\".format(first_training_year, last_training_year, validation_year, prediction_year))\n",
    "elif (first_training_year < validation_year < last_training_year):\n",
    "    print(\"--> Training years  = {}-{} except {}\".format(first_training_year, last_training_year, validation_year))\n",
    "elif (first_training_year < prediction_year < last_training_year):\n",
    "    print(\"--> Training years  = {}-{} except {}\".format(first_training_year, last_training_year, prediction_year))\n",
    "else:\n",
    "    print(\"--> Training years  = {}-{}\".format(first_training_year, last_training_year))\n",
    "print(\"\")\n",
    "\n",
    "if includeadvancedstats:\n",
    "    print(\"--> Advanced statistics included\")\n",
    "else:\n",
    "    print(\"--> Advanced statistics not included\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NBA player data:\n",
    "\n",
    "NBA player data from 2000-2018 from https://www.basketball-reference.com have been saved as csv-files in the *data* directory using the scraper functions (*NBA_per_game_scraper*, *NBA_advanced_scraper* and *NBA_per_game_scraper*) in NBAanalysissetup.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training, df_validation, df_prediction = \\\n",
    "NBAanalysissetup.loaddata_allyears(prediction_year, validation_year, training_years, includeadvancedstats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data and create features (X) and target (y) dataframes needed for sklearn routines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove players which have NAN(s):\n",
    "\n",
    "df_training   = df_training.  dropna()\n",
    "df_validation = df_validation.dropna()\n",
    "df_prediction = df_prediction.dropna()\n",
    "\n",
    "# Remove AS feature (binary variable indicating All-Star selection) since this is what we want to predict\n",
    "# Remove YEAR feature since this is only used for the cross-validation scheme\n",
    "# Remove features which do not have predictive power (e.g. Player name, Team name, etc.) \n",
    "# Remove features which are correlated with other features (e.g. FGA, 3PA, etc.)\n",
    "\n",
    "X_training    = df_training.  drop(['AS', 'YEAR', 'Player', 'Pos', 'Tm', 'Age',\n",
    "                                    'FGA/G', '3PA/G', '2PA/G', 'FTA/G', 'TRB/G'], axis=1)\n",
    "X_validation  = df_validation.drop(['AS', 'YEAR', 'Player', 'Pos', 'Tm', 'Age',\n",
    "                                    'FGA/G', '3PA/G', '2PA/G', 'FTA/G', 'TRB/G'], axis=1)\n",
    "X_prediction  = df_prediction.drop(['AS', 'YEAR', 'Player', 'Pos', 'Tm', 'Age',\n",
    "                                    'FGA/G', '3PA/G', '2PA/G', 'FTA/G', 'TRB/G'], axis=1)\n",
    "\n",
    "if includeadvancedstats:\n",
    "\n",
    "    X_training    = X_training.  drop(['WS'], axis=1)\n",
    "    X_validation  = X_validation.drop(['WS'], axis=1)\n",
    "    X_prediction  = X_prediction.drop(['WS'], axis=1)\n",
    "    \n",
    "# The target is the AS feature:\n",
    "    \n",
    "y_training    = df_training  ['AS']\n",
    "y_validation  = df_validation['AS']\n",
    "y_prediction  = df_prediction['AS']\n",
    "\n",
    "print(\"--> Training   data set      : # of players = {}, # of features = {}\".format(X_training.shape[0], X_training.shape[1]))\n",
    "print(\"--> Validation data set {} : # of players = {:4}, # of features = {}\".format(validation_year, X_validation.shape[0], X_validation.shape[1]))\n",
    "print(\"--> Prediction data set {} : # of players = {:4}, # of features = {}\".format(prediction_year, X_prediction.shape[0], X_prediction.shape[1]))\n",
    "print(\"\")\n",
    "\n",
    "print(\"--> Model features : \", list(X_training.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select ML classifier, set hyper-parameters and instantiate model:\n",
    "\n",
    "Choose one of the 12 classifiers in the list below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = 1\n",
    "\n",
    "if (classifier == 1):\n",
    "    from sklearn import linear_model\n",
    "    C = 0.4  # smaller value for C results in more regularization (in case you have noisy observations)\n",
    "    p = 'l2' # penalty=\"l1\" enables Lasso regularization, penalty=\"l2\" enables Ridge regularization. Ridge gives Shrinkage (i.e. non-sparse coefficients), Lasso gives Sparsity (i.e. prefer simpler models)\n",
    "    model = linear_model.LogisticRegression(C=C, penalty=p)\n",
    "    modelname = 'Logistic Regression Classifier'\n",
    "elif (classifier == 2):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    model = GaussianNB()\n",
    "    modelname = 'Gaussian Naive Bayes Classifier'\n",
    "elif (classifier == 3):\n",
    "    from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "    model = GaussianProcessClassifier() #(1.0 * RBF(1.0))\n",
    "    modelname = 'Gaussian Process Classifier'\n",
    "elif (classifier == 4):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    n = 10\n",
    "    model = KNeighborsClassifier(n_neighbors=n)\n",
    "    modelname = 'Nearest Neighbours Classifier'\n",
    "elif (classifier == 5):\n",
    "    from sklearn.svm import LinearSVC                \n",
    "    #from sklearn.svm import SVC                \n",
    "    C = 0.05 # See Logistic Regression\n",
    "    p = 'l2' # See Logistic Regression\n",
    "    model = LinearSVC(C=C, penalty=p, dual=False)\n",
    "    #model = SVC(kernel='linear', C=C, class_weight='balanced') #(gamma=2)\n",
    "    modelname = 'Linear Support Vector Machine Classifier'\n",
    "elif (classifier == 6):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    m = None\n",
    "    model = DecisionTreeClassifier(max_depth=m)\n",
    "    modelname = 'Decision Tree Classifier'\n",
    "elif (classifier == 7):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    m = None\n",
    "    n = 100\n",
    "    f = 'auto'\n",
    "    model = RandomForestClassifier(n_estimators=n, max_depth=m, max_features=f) #, class_weight='balanced'\n",
    "    modelname = 'Random Forest Classifier'\n",
    "elif (classifier == 8):\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    m = None\n",
    "    n = 100\n",
    "    model = ExtraTreesClassifier(max_depth=m, n_estimators=n, min_samples_split=2, random_state=0)\n",
    "    modelname = 'Extra Trees Classifier'\n",
    "elif (classifier == 9):\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    n = 100\n",
    "    m = None\n",
    "    l = 0.1\n",
    "    model = GradientBoostingClassifier(n_estimators=n, max_depth=m, learning_rate=l)\n",
    "    modelname = 'Gradient Tree Boosting Classifier'\n",
    "elif (classifier == 10):\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    n = 100\n",
    "    l = 0.1\n",
    "    model = AdaBoostClassifier(n_estimators=n, learning_rate=l)\n",
    "    modelname = 'Ada Boost Classifier'\n",
    "elif (classifier == 11):\n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "    model = QuadraticDiscriminantAnalysis()\n",
    "    modelname = 'Quadratic Discriminant Analysis Classifier'\n",
    "elif (classifier == 12):\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    a = 1\n",
    "    model = MLPClassifier(alpha=a)\n",
    "    modelname = 'Neural Network Classifier'\n",
    "else:\n",
    "    print(\"That number does not correspond to an implemented classifier - EXIT\")\n",
    "    \n",
    "print(\"--> Selected classifier =\", modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Cross-validate the model using training data and the *LeaveOneGroupOut* cross-validation scheme, and calculate some model scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, LeaveOneGroupOut\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "cv_groups = df_training['YEAR'] # Use YEAR to define groups in LeaveOneGroupOut cross-validation scheme\n",
    "\n",
    "cv_logo = logo.split(X_training, y_training, groups=cv_groups)\n",
    "\n",
    "scoring_list = ['precision', 'recall', 'f1', 'accuracy', 'roc_auc']\n",
    "\n",
    "scores = cross_validate(model, X_training, y_training, cv=cv_logo, scoring=scoring_list, n_jobs=-1)\n",
    "\n",
    "print(\"--> Cross-val years  :\", [\"{:5d}\".format(yr) for yr in training_years])\n",
    "print(\"--> Precision scores :\", [\"{:5.2f}\".format(i) for i in scores['test_precision']])\n",
    "print(\"--> Recall    scores :\", [\"{:5.2f}\".format(i) for i in scores['test_recall'   ]])\n",
    "print(\"--> F1        scores :\", [\"{:5.2f}\".format(i) for i in scores['test_f1'       ]])\n",
    "print(\"--> Accuracy  scores :\", [\"{:5.2f}\".format(i) for i in scores['test_accuracy' ]])\n",
    "print(\"--> ROC-AUC   scores :\", [\"{:5.2f}\".format(i) for i in scores['test_roc_auc'  ]])\n",
    "print(\"\")\n",
    "print(\"--> Precision score : {:5.1%} +/- {:5.1%}\".format(np.mean(scores['test_precision']), np.std(scores['test_precision'])))\n",
    "print(\"--> Recall score    : {:5.1%} +/- {:5.1%}\".format(np.mean(scores['test_recall'   ]), np.std(scores['test_recall'   ])))\n",
    "print(\"--> F1 score        : {:5.1%} +/- {:5.1%}\".format(np.mean(scores['test_f1'       ]), np.std(scores['test_f1'       ])))\n",
    "print(\"--> Accuracy score  : {:5.1%} +/- {:5.1%}\".format(np.mean(scores['test_accuracy' ]), np.std(scores['test_accuracy' ])))\n",
    "print(\"--> ROC-AUC score   : {:5.1%} +/- {:5.1%}\".format(np.mean(scores['test_roc_auc'  ]), np.std(scores['test_roc_auc'  ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Fit model to training data, use fitted model to predict validation data and calculate the corresponding confusion matrix and some model scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_training, y_training)     # Fit model to training data\n",
    "\n",
    "y_model = model.predict(X_validation) # Predict validation data\n",
    "\n",
    "y_valtrue  = y_validation.tolist()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "CM = confusion_matrix(y_valtrue, y_model) # defined as: rows -> true, columns -> prediction\n",
    "\n",
    "print(\"--> Confusion matrix {}:\".format(validation_year))\n",
    "print(CM)\n",
    "print(\"\")\n",
    "\n",
    "TN = CM[0,0] # defined as: 0 = negative, 1 = positive\n",
    "FN = CM[1,0] # defined as: 0 = negative, 1 = positive\n",
    "FP = CM[0,1] # defined as: 0 = negative, 1 = positive\n",
    "TP = CM[1,1] # defined as: 0 = negative, 1 = positive\n",
    "\n",
    "TOT = TP + FP + FN + TN\n",
    "\n",
    "print(\"--> TP = {}, FP = {}, FN = {}, TN = {}\".format(TP, FP ,FN, TN))\n",
    "print(\"\")\n",
    "print(\"--> True  Positive Rate i.e. Recall   (TP/(TP+FN)) = {:5.1%}\".format(TP/(TP+FN)))\n",
    "print(\"--> False Positive Rate i.e. Fall-Out (FP/(FP+TN)) = {:5.1%}\".format(FP/(FP+TN)))\n",
    "print(\"\")\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "precision = precision_score(y_validation, y_model)\n",
    "recall    = recall_score   (y_validation, y_model)\n",
    "f1        = f1_score       (y_validation, y_model)\n",
    "accuracy  = accuracy_score (y_validation, y_model)\n",
    "roc_auc   = roc_auc_score  (y_validation, y_model)\n",
    "        \n",
    "print(\"--> Precision score : {:.1%}\".format(precision))\n",
    "print(\"--> Recall score    : {:.1%}\".format(recall   ))\n",
    "print(\"--> F1 score        : {:.1%}\".format(f1       ))\n",
    "print(\"--> Accuracy score  : {:.1%}\".format(accuracy ))\n",
    "print(\"--> ROC-AUC score   : {:.1%}\".format(roc_auc  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Calculate ROC and PR curves using validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "decisionfunctionclassifiers = [1, 5] # LogisticRegression, SVC\n",
    "\n",
    "if (classifier in decisionfunctionclassifiers):\n",
    "    y_score = model.decision_function(X_validation)\n",
    "else:\n",
    "    y_score = model.predict_proba(X_validation)\n",
    "\n",
    "if (classifier in decisionfunctionclassifiers):\n",
    "    fpr, tpr, thresholds = roc_curve(y_validation, y_score)       # to be used when y_score is calculated using decision_function method\n",
    "else:\n",
    "    fpr, tpr, thresholds = roc_curve(y_validation, y_score[:, 1]) # to be used when y_score is calculated using predict_proba method\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='{} (AUC={:.2f})'.format(modelname, roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', label='Random classifier')\n",
    "plt.xlabel('False Positive Rate i.e. Fall-Out')\n",
    "plt.ylabel('True Positive Rate i.e. Recall')\n",
    "plt.title('Receiver Operating Characteristic curve {}'.format(validation_year))\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.text(0.65, 0.3, r\"ROC-AUC = {:.2f}\".format(roc_auc), color='red')\n",
    "plt.grid(True)\n",
    "\n",
    "if (classifier in decisionfunctionclassifiers):\n",
    "    precision, recall, _ = precision_recall_curve(y_validation, y_score)       # to be used when y_score is calculated using decision_function method\n",
    "else:\n",
    "    precision, recall, _ = precision_recall_curve(y_validation, y_score[:, 1]) # to be used when y_score is calculated using predict_proba method\n",
    "\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='green', lw=2, label='{} (AUC={:.2f})'.format(modelname, pr_auc))\n",
    "plt.plot([0, 1], [1, 0], color='black', linestyle='--', label='Random classifier')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve {}'.format(validation_year))\n",
    "plt.legend(loc=\"lower left\")\n",
    "#plt.text(0.05, 0.3, r\"PR-AUC = {:.2f}\".format(pr_auc), color='green')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the feature coefficients and importances of the fitted model, if applicable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(model, \"coef_\"):\n",
    "    print(\"--> Model coefficients: \")  #, model.coef_.ravel())\n",
    "    print(\"\")\n",
    "    for name, coef in zip(X_training.columns, model.coef_.ravel()):\n",
    "        print(\"----> Model coefficient {:5} = {:>6.3f}\".format(name, coef))\n",
    "    \n",
    "if hasattr(model, \"feature_importances_\"):\n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    print(\"--> Feature importances: \") #, model.feature_importances_.ravel())\n",
    "    print(\"\")\n",
    "    \n",
    "    for i in range(X_training.shape[1]):\n",
    "        print(\"----> Feature importance ({:>2}) {:5} : {:.3f}\".format(i + 1, X_training.columns[indices[i]], importances[indices[i]]))\n",
    "    #for name, imp in zip(X_training.columns, model.feature_importances_):\n",
    "    #    print(\"----> Feature importance {:4} : {:.3f}\".format(name, imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions\n",
    "\n",
    "Use fitted model to predict the NBA All-Stars in *prediction_year*, and calculate the corresponding confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = model.predict(X_prediction) # Use fitted model on prediction data\n",
    "\n",
    "y_true  = y_prediction.tolist()\n",
    "\n",
    "CM = confusion_matrix(y_true, y_model) # defined as: rows -> true, columns -> prediction\n",
    "\n",
    "print(\"--> Confusion matrix {}:\".format(prediction_year))\n",
    "print(CM)\n",
    "print(\"\")\n",
    "\n",
    "TN = CM[0,0] # defined as: 0 = negative, 1 = positive\n",
    "FN = CM[1,0] # defined as: 0 = negative, 1 = positive\n",
    "FP = CM[0,1] # defined as: 0 = negative, 1 = positive\n",
    "TP = CM[1,1] # defined as: 0 = negative, 1 = positive\n",
    "\n",
    "TOT = TP + FP + FN + TN\n",
    "\n",
    "print(\"--> TP = {}, FP = {}, FN = {}, TN = {}\".format(TP, FP ,FN, TN))\n",
    "print(\"\")\n",
    "\n",
    "note1 = \" (Answers the question: Out of all true All-Stars, how many are predicted?)\"\n",
    "note2 = \" (Answers the question: Out of all predicted All-Stars, how many are true?)\"\n",
    "note3 = \" (i.e. the harmonic mean of Precision and Recall)\"\n",
    "note4 = \" (Answers the question: Out of all players, how many are correctly predicted?)\"\n",
    "    \n",
    "print(\"--> Precision (TP/(TP+FP)) = {:5.1%}\".format(TP/(TP+FP))         + note1)\n",
    "print(\"--> Recall    (TP/(TP+FN)) = {:5.1%}\".format(TP/(TP+FN))         + note2)\n",
    "print(\"--> F1 score               = {:5.1%}\".format(2*TP/(2*TP+FP+FN))  + note3)\n",
    "print(\"--> Accuracy ((TP+TN)/TOT) = {:5.1%}\".format((TP+TN)/TOT)        + note4) \n",
    "    \n",
    "np.set_printoptions(precision=2)\n",
    "class_names = ['Non-All-Star','All-Star']\n",
    "\n",
    "plt.figure()\n",
    "NBAanalysissetup.plot_confusion_matrix(CM, classes=class_names,\n",
    "                                       title='Confusion matrix {}'.format(prediction_year))\n",
    "\n",
    "#plt.figure()\n",
    "#NBAanalysissetup.plot_confusion_matrix(CM, classes=class_names, normalize=True,\n",
    "#                                       title='Normalized confusion matrix')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"--> Classification report {}:\".format(prediction_year))\n",
    "print(\"\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "class_names = ['Non-All-Stars (true)', 'All-Stars (true)']\n",
    "print(classification_report(y_true, y_model, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which players are All-Stars according to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [0, 0, 0, 0]\n",
    "\n",
    "snubbed      = []\n",
    "deserved     = []\n",
    "questionable = []\n",
    "    \n",
    "for i in range(0, len(y_model)):\n",
    "    if ((y_true[i] == 0) and (y_model[i] == 0)):    # TN\n",
    "        counts[0] += 1\n",
    "    elif ((y_true[i] == 0) and (y_model[i] == 1)):  # FP\n",
    "        counts[1] += 1\n",
    "        snubbed.append(df_prediction.iat[i,0])              # 0-th column in df is player name\n",
    "    elif ((y_true[i] == 1) and (y_model[i] == 0)):  # FN\n",
    "        counts[2] += 1\n",
    "        questionable.append(df_prediction.iat[i,0])         # 0-th column in df is player name\n",
    "    else:                                           # TP\n",
    "        counts[3] += 1\n",
    "        deserved.append(df_prediction.iat[i,0])             # 0-th column in df is player name\n",
    "        \n",
    "print(\"--> # of     All-Stars predicted to be     All-Stars = {:>3} (TP)\".format(counts[3]))\n",
    "print(\"--> # of     All-Stars predicted to be non-All-Stars = {:>3} (FN)\".format(counts[2]))\n",
    "print(\"--> # of non-All-Stars predicted to be     All-Stars = {:>3} (FP)\".format(counts[1]))\n",
    "print(\"--> # of non-All-Stars predicted to be non-All-Stars = {:>3} (TN)\".format(counts[0]))\n",
    "print(\"\")\n",
    "print(\"--> Deserved true All-Stars:     \", deserved)\n",
    "print(\"\")\n",
    "print(\"--> Questionable true All-Stars: \", questionable)\n",
    "print(\"\")\n",
    "print(\"--> Snubbed non-All-Stars:       \", snubbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all NBA players in *prediction_year* according to their model scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--> Model scores for all players in {}:\".format(prediction_year))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "if (classifier in decisionfunctionclassifiers):\n",
    "    y_score = model.decision_function(X_prediction)\n",
    "else:\n",
    "    y_score = model.predict_proba(X_prediction)\n",
    "\n",
    "\n",
    "#y_score = model.decision_function(X_prediction)\n",
    "#y_score = model.predict_proba(X_prediction)\n",
    "\n",
    "player_score_dict = {}\n",
    "player_AS_dict    = {}\n",
    "\n",
    "if includeadvancedstats:\n",
    "    AS_index = 49\n",
    "else:\n",
    "    AS_index = 29\n",
    "    \n",
    "for i in range(0, len(y_model)):\n",
    "    if (classifier in decisionfunctionclassifiers):\n",
    "        player_score_dict[df_prediction.iat[i,0]] = y_score[i].ravel()[0]\n",
    "    else:\n",
    "        player_score_dict[df_prediction.iat[i,0]] = y_score[i].ravel()[1]\n",
    "    if df_prediction.iat[i,AS_index] > 0.5:\n",
    "        status = 'All-Star'\n",
    "    else:\n",
    "        status = 'Non-All-Star'\n",
    "    player_AS_dict[df_prediction.iat[i,0]] = status\n",
    "        \n",
    "import operator\n",
    "sorted_player_score_dict = sorted(player_score_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "counter = 0\n",
    "printed_line = False\n",
    "for key, value in dict(sorted_player_score_dict).items():\n",
    "    counter += 1\n",
    "    if (classifier in decisionfunctionclassifiers):\n",
    "        if (value < 0 and not printed_line):\n",
    "            print(\"*******************************************\")\n",
    "            printed_line = True\n",
    "    else:\n",
    "        if (value < 0.5 and not printed_line):\n",
    "            print(\"*******************************************\")\n",
    "            printed_line = True\n",
    "    print(\"----> {:3}: {:24} = {:.3f} ({})\".format(counter, key, value, player_AS_dict[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
